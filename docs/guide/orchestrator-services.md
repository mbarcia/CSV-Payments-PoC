# Creating Orchestrator Services

This guide explains how to create orchestrator services using The Pipeline Framework, which coordinate and execute the complete pipeline of backend services.

<Callout type="tip" title="Visual Orchestrator Configuration">
Use the Canvas designer at <a href="https://app.pipelineframework.org" target="_blank">https://app.pipelineframework.org</a> to visually configure your orchestrator services. The Canvas allows you to define the complete pipeline flow, including input sources, step connections, and output handlers, without writing complex orchestration code.
</Callout>

## Overview

Orchestrator services are responsible for:
1. Initiating the pipeline execution
2. Providing input data to the pipeline
3. Coordinating the flow between pipeline steps
4. Handling the final output of the pipeline

The Pipeline Framework automatically generates the core pipeline execution logic when backend services are annotated with `@PipelineStep`, leaving orchestrator services to focus on input provisioning and output handling.

## Orchestrator Service Creation

### 1. Extend PipelineApplication

Create your orchestrator service by extending the framework's `PipelineApplication`:

```java
// orchestrator-svc/src/main/java/com/example/app/orchestrator/CsvPaymentsApplication.java
@QuarkusMain
@CommandLine.Command(
    name = "csv-payments",
    mixinStandardHelpOptions = true,
    version = "1.0.0",
    description = "Process CSV payment files")
public class CsvPaymentsApplication extends PipelineApplication implements Runnable, QuarkusApplication {

    @Inject
    ProcessFolderService processFolderService;

    @Inject
    SystemExiter exiter;

    @CommandLine.Option(
        names = {"-c", "--csv-folder"},
        description = "The folder path containing CSV payment files",
        defaultValue = "${env:CSV_FOLDER_PATH:-csv/}")
    String csvFolder;

    public static void main(String[] args) {
        Quarkus.run(CsvPaymentsApplication.class, args);
    }

    @Override
    public int run(String... args) {
        return new CommandLine(this).execute(args);
    }

    @Override
    public void run() {
        processPipeline(csvFolder);
    }
}
```

### 2. Implement Input Provisioning

Provide the initial input data for the pipeline:

```java
@Override
public void processPipeline(String input) {
    LOG.info("Starting pipeline processing for folder: {}", input);

    try {
        // Convert input (folder path) to stream of domain objects
        Stream<CsvPaymentsInputFile> inputFileStream = processFolderService.process(input);
        
        // Convert to reactive Multi
        Multi<CsvPaymentsInputFile> inputMulti = Multi.createFrom().iterable(inputFileStream::iterator);
        
        // Execute the pipeline with the input
        executePipeline(inputMulti, List.of(/* steps - automatically populated by annotation processor */));
        
    } catch (Exception e) {
        LOG.error("Error during pipeline processing", e);
        exiter.exit(1);
    }
}
```

## Understanding Generated Pipeline Applications

When backend services are annotated with `@PipelineStep`, The Pipeline Framework's annotation processor automatically generates a `GeneratedPipelineApplication` class that:

1. Extends your orchestrator service
2. Automatically discovers all `@PipelineStep` annotated services
3. Creates and injects the required step instances
4. Builds the complete pipeline in the correct order

The generated application looks something like this:

```java
// This class is auto-generated by the annotation processor
@ApplicationScoped
public class GeneratedPipelineApplication extends CsvPaymentsApplication {
    
    @Inject
    Step1 step1;
    
    @Inject
    Step2 step2;
    
    @Inject
    Step3 step3;
    
    @Override
    public void processPipeline(String input) {
        // Call parent method for input processing
        super.processPipeline(input);
    }
    
    @Override
    protected void executePipeline(Multi<?> input, List<Object> steps) {
        // Add all discovered steps to the pipeline
        List<Object> allSteps = new ArrayList<>(steps);
        allSteps.addAll(Arrays.asList(step1, step2, step3));
        
        // Delegate to parent implementation
        super.executePipeline(input, allSteps);
    }
}
```

## Input Processing Service

Create a service to handle input data preparation:

```java
// orchestrator-svc/src/main/java/com/example/app/orchestrator/service/ProcessFolderService.java
@ApplicationScoped
public class ProcessFolderService {

    @Inject
    HybridResourceLoader resourceLoader;

    public Stream<CsvPaymentsInputFile> process(String csvFolderPath) throws URISyntaxException {
        LOG.info("Reading CSV folder from path: {}", csvFolderPath);

        URL resource = resourceLoader.getResource(csvFolderPath);
        if (resource == null) {
            LOG.warn("CSV folder not found: {}", csvFolderPath);
            resourceLoader.diagnoseResourceAccess(csvFolderPath);
            throw new IllegalArgumentException(
                MessageFormat.format("CSV folder not found: {0}", csvFolderPath));
        }

        File directory = new File(resource.toURI());
        if (!directory.exists() || !directory.isDirectory()) {
            throw new IllegalArgumentException(
                MessageFormat.format(
                    "CSV path is not a valid directory: {0}", directory.getAbsolutePath()));
        }

        File[] csvFiles = directory.listFiles((_, name) -> name.toLowerCase().endsWith(".csv"));
        if (csvFiles == null || csvFiles.length == 0) {
            LOG.warn("No CSV files found in {}", csvFolderPath);
            resourceLoader.diagnoseResourceAccess(csvFolderPath);
            return Stream.empty();
        }

        return Stream.of(csvFiles)
            .map(CsvPaymentsInputFile::new);
    }
}
```

## Pipeline Execution Flow

### 1. Input Preparation
The orchestrator prepares the initial input data:

```java
@Override
public void processPipeline(String input) {
    LOG.info("Starting pipeline processing for folder: {}", input);

    try {
        // Step 1: Prepare input data
        Stream<CsvPaymentsInputFile> inputFileStream = processFolderService.process(input);
        
        // Step 2: Convert to reactive stream
        Multi<CsvPaymentsInputFile> inputMulti = Multi.createFrom().iterable(inputFileStream::iterator);
        
        // Step 3: Execute pipeline (steps automatically injected by generated application)
        executePipeline(inputMulti, List.of());
        
    } catch (Exception e) {
        handleError(e);
    }
}
```

### 2. Pipeline Execution
The framework executes the pipeline with all discovered steps:

```java
protected void executePipeline(Multi<?> input, List<Object> steps) {
    LOG.info("PIPELINE BEGINS processing");

    StopWatch watch = new StopWatch();
    watch.start();

    // Configure pipeline profiles
    pipelineConfig.defaults().retryLimit(3).debug(false);
    pipelineConfig.profile("dev", new StepConfig().retryLimit(1).debug(true));
    pipelineConfig.profile("prod", new StepConfig().retryLimit(5).retryWait(Duration.ofSeconds(1)));

    try {
        Object result = pipelineRunner.run(
                input,
                steps  // This includes both provided steps and auto-discovered steps
        );

        Multi<?> multiResult;
        if (result instanceof Multi) {
            multiResult = (Multi<?>) result;
        } else if (result instanceof Uni) {
            multiResult = ((Uni<?>) result).toMulti();
        } else {
            throw new IllegalStateException("PipelineRunner returned unexpected type: " + result.getClass());
        }

        multiResult
            .subscribe()
            .with(
                _ -> {
                    LOG.info("Processing completed.");
                    watch.stop();
                    LOG.info(
                        "✅ PIPELINE FINISHED processing in {} seconds",
                        watch.getTime(TimeUnit.SECONDS));
                    signalCompletion();
                },
                failure -> {
                    LOG.error(MessageFormat.format("Error: {0}", failure.getMessage()));
                    watch.stop();
                    LOG.error(
                        "❌ PIPELINE FAILED after {} seconds",
                        watch.getTime(TimeUnit.SECONDS),
                        failure);
                    signalFailure(failure);
                });

    } catch (Exception e) {
        watch.stop();
        LOG.error(
            "❌ PIPELINE ABORTED after {} seconds",
            watch.getTime(TimeUnit.SECONDS),
            e);
        signalFailure(e);
    }
}
```

## Command Line Interface

### Options Configuration
Configure command line options for your orchestrator:

```java
@CommandLine.Option(
    names = {"-c", "--csv-folder"},
    description = "The folder path containing CSV payment files",
    defaultValue = "${env:CSV_FOLDER_PATH:-csv/}")
String csvFolder;

@CommandLine.Option(
    names = {"-d", "--debug"},
    description = "Enable debug mode",
    defaultValue = "false")
boolean debugMode;

@CommandLine.Option(
    names = {"-p", "--profile"},
    description = "Active profile (dev, prod, etc.)",
    defaultValue = "dev")
String profile;
```

### Help Documentation
Provide clear help documentation:

```java
@CommandLine.Command(
    name = "csv-payments",
    mixinStandardHelpOptions = true,
    version = "1.0.0",
    description = "Process CSV payment files through a reactive pipeline")
public class CsvPaymentsApplication extends PipelineApplication {
    // Implementation
}
```

## Error Handling

### Graceful Error Handling
Handle errors gracefully in your orchestrator:

```java
private void handleError(Exception e) {
    LOG.error("Pipeline execution failed", e);
    
    // Determine exit code based on error type
    int exitCode = 1;
    if (e instanceof IllegalArgumentException) {
        exitCode = 2;  // Bad input
    } else if (e instanceof FileNotFoundException) {
        exitCode = 3;  // File not found
    }
    
    // Log error details
    LOG.error("Exiting with code: {}", exitCode);
    
    // Exit gracefully
    exiter.exit(exitCode);
}

private void signalCompletion() {
    LOG.info("Pipeline execution completed successfully");
    exiter.exit(0);
}

private void signalFailure(Throwable error) {
    LOG.error("Pipeline execution failed: {}", error.getMessage(), error);
    exiter.exit(1);
}
```

## Testing

### Unit Testing
Test your orchestrator service logic:

```java
@QuarkusTest
class CsvPaymentsApplicationTest {

    @InjectMock
    ProcessFolderService processFolderService;

    @InjectMock
    SystemExiter exiter;

    @Test
    void testProcessPipelineWithValidInput() throws Exception {
        // Arrange
        String testFolder = "test-folder";
        List<CsvPaymentsInputFile> testFiles = createTestFiles();
        
        when(processFolderService.process(testFolder))
            .thenReturn(testFiles.stream());

        // Act
        CsvPaymentsApplication app = new CsvPaymentsApplication();
        app.processPipeline(testFolder);

        // Assert
        verify(processFolderService).process(testFolder);
        // Additional assertions...
    }

    @Test
    void testProcessPipelineWithInvalidInput() throws Exception {
        // Arrange
        String invalidFolder = "invalid-folder";
        when(processFolderService.process(invalidFolder))
            .thenThrow(new IllegalArgumentException("Folder not found"));

        // Act & Assert
        assertThrows(IllegalArgumentException.class, () -> {
            CsvPaymentsApplication app = new CsvPaymentsApplication();
            app.processPipeline(invalidFolder);
        });
    }

    private List<CsvPaymentsInputFile> createTestFiles() {
        // Create test data
        return Arrays.asList(/* test files */);
    }
}
```

### Integration Testing
Test the complete pipeline execution:

```java
@QuarkusTest
class PipelineExecutionIntegrationTest {

    @Test
    void testFullPipelineExecution() {
        // Test the complete pipeline with all steps
        // This would involve setting up test data and verifying the output
    }
}
```

## Configuration

### Application Properties
Configure orchestrator behavior through application properties:

```properties
# application.properties
# Pipeline configuration
csv-poc.pipeline.retry-limit=3
csv-poc.pipeline.retry-wait-ms=500
csv-poc.pipeline.concurrency=2000
csv-poc.pipeline.debug=false
csv-poc.pipeline.recover-on-failure=false
csv-poc.pipeline.run-with-virtual-threads=true
csv-poc.pipeline.auto-persist=true
csv-poc.pipeline.max-backoff-ms=30000
csv-poc.pipeline.jitter=false

# gRPC clients
quarkus.grpc.clients.process-csv-payments-input-file.host=localhost
quarkus.grpc.clients.process-csv-payments-input-file.port=8444
quarkus.grpc.clients.process-csv-payments-input-file.plain-text=false
quarkus.grpc.clients.process-csv-payments-input-file.use-quarkus-grpc-client=true
quarkus.grpc.clients.process-csv-payments-input-file.tls.enabled=true
```

### Environment Variables
Use environment variables for configuration:

```bash
# Set CSV folder path
export CSV_FOLDER_PATH="/path/to/csv/files"

# Set pipeline configuration
export PIPELINE_RETRY_LIMIT=5
export PIPELINE_DEBUG=true
```

## Best Practices

### Design Principles
1. **Separation of Concerns**: Keep orchestrator focused on coordination
2. **Input Flexibility**: Support multiple input sources (files, databases, APIs)
3. **Output Handling**: Provide clear output handling and reporting
4. **Error Resilience**: Handle failures gracefully with proper error reporting

### Performance Considerations
1. **Efficient Input Processing**: Process input data efficiently
2. **Memory Management**: Manage memory usage for large datasets
3. **Concurrency Control**: Control pipeline concurrency appropriately
4. **Resource Cleanup**: Ensure proper resource cleanup

### Observability
1. **Structured Logging**: Use structured logging with trace IDs
2. **Metrics Collection**: Collect and expose pipeline metrics
3. **Health Checks**: Implement health check endpoints
4. **Tracing**: Enable distributed tracing for pipeline steps

### Deployment
1. **Containerization**: Package as Docker containers
2. **Configuration Management**: Use externalized configuration
3. **Monitoring**: Implement monitoring and alerting
4. **Scaling**: Design for horizontal scaling when needed